{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/W138923/GPT4-hackathon/blob/main/training_TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG_EcOaBcQpe"
      },
      "outputs": [],
      "source": [
        "\\import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "from pandas import DataFrame as df\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2cSOEGwM8uZ"
      },
      "source": [
        "# **0: envrioment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwL24CJiNdLZ"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://www.alphavantage.co/query?function=FX_INTRADAY&from_symbol=JPY&to_symbol=USD&interval=60min&apikey=N914VF8FF269Z2ZJ&outputsize=full&datatype=csv'\n",
        "r = requests.get(url)\n",
        "import csv\n",
        "data = []\n",
        "for row in csv.reader(r.text.strip().splitlines()):\n",
        "  data.append(row)\n",
        "df(data[1:],columns=data[0])\n",
        "map = df(data[1:],columns=data[0])\n",
        "map = map.rename(columns={'open':'Open',\n",
        "                    'high': 'High',\n",
        "                    'low': 'Low',\n",
        "                    'close': 'Close'})\n",
        "map = map.drop('timestamp',axis=1)\n",
        "for x in map.columns:\n",
        "map.iloc[:,1:] = map.iloc[:,1:].astype('float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8VSjFPB6wMu"
      },
      "source": [
        "from alpha_vantage.foreignexchange import ForeignExchange\n",
        "ts = ForeignExchange(key='N914VF8FF269Z2ZJ')\n",
        "data, meta_data = ts.get_currency_exchange_intraday('USD','EUR',interval='1min', outputsize='full')\n",
        "map = df(data).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vI9tE8lDho-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6655065e-4307-4b77-f8b1-26724901432a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "map = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DeepL/EN/TD3/USD_JPY Historical Data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lrU55WIqCZs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "17a41cee-0abb-450c-808d-f36eef20b60f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-be9b1733e361>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'open'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Open'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'high'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'High'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'low'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Low'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Close'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vol.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Change %'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'map' has no attribute 'rename'"
          ]
        }
      ],
      "source": [
        "map = map.rename(columns={'open':'Open', 'high': 'High', 'low': 'Low', 'close': 'Close'}) \n",
        "map = map.drop('Date',axis=1) \n",
        "map = map.drop('Vol.',axis=1) \n",
        "map = map.drop('Change %',axis=1) \n",
        "\n",
        "for x in map.columns: \n",
        "  map.iloc[:,1:] = map.iloc[:,1:].astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "example = (5 - -5) * np.random.rand(10) + -5\n",
        "_current_asset = 0\n",
        "_account_balance = 1\n",
        "test_df = map.iloc[:30,1].values\n",
        "trade_asset =  t_asset * test_df[x]\n",
        "for x in range(10):\n",
        "  action = example[x]\n",
        "  t_asset = action * _account_balance\n",
        "  step_profit = 0 \n",
        "  if _last_trade_price != 0: none\n",
        "  \n",
        "  _account_balance+=step_profit"
      ],
      "metadata": {
        "id": "9BFyzX-aYaNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_oLPlmWNGKS"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class env_trade(gym.Env):\n",
        "  metadata = {'render.modes': ['human']}\n",
        "  # due to limitation of my programming skills, I will only allow one position to be existing at the time. Meaning, position will be reloved when another trade is done.\n",
        "  # this only works, if the comoddity price is more then 0.\n",
        "\n",
        "  def __init__(self, df, window_size=30,account_balance=1,max_leverage=5):\n",
        "      # In order to add function to use mutiple df, df2 and df3 will be here, but unused.\n",
        "    # what should be remembered?\n",
        "    # parameter\n",
        "    self._initial_balance=account_balance\n",
        "    self._max_leverage = max_leverage \n",
        "    self.window_size = window_size\n",
        "    self._sell_commission = 0.005\n",
        "    self._leverage_commission = 0.001 # it will not be used for now.\n",
        "    self._trade_commission = 0.005\n",
        "    self._max_episode_steps= len(map)-window_size\n",
        "    self.df = df\n",
        "    self.signal_features=self.df.loc[:,'Price']\n",
        "    # initial values\n",
        "    self._account_balance= self._initial_balance\n",
        "    self._invested = 0\n",
        "    self._total_profit = 0 \n",
        "    self._current_tick = self.window_size\n",
        "    self._position_history = []\n",
        "    self.history = None\n",
        "    # spaces\n",
        "    self.action_space = gym.spaces.Box(low=-(max_leverage), high=max_leverage, shape=(1,))\n",
        "                                              #[0,1] )#0 is nothing, 1 is resolve previous trade\n",
        "                                              #[0,self.map_number] #map number could be changed. and [0] is trade, also add code to change account_balance each time\n",
        "    self.observation_space = gym.spaces.Box(low=self.df.min()['Low'], high=self.df.max()['High'], shape=[window_size,], dtype=np.float32) # maybe add observation about last trade\n",
        "    \n",
        "  \n",
        "  def step(self, action):\n",
        "    #prevent buying asset when account balance is negative.\n",
        "    print(\"starting cash: %f\" % (self._account_balance),\"starting assets: %f\" % (self._invested))\n",
        "    step_profit = 0\n",
        "    if self._account_balance <= 0 and action>=0:\n",
        "      step_profit,t_asset = 0,0\n",
        "    else:\n",
        "      step_profit,self._invested,t_asset = self._trade(action)\n",
        "      self._total_profit += step_profit\n",
        "      self._account_balance += step_profit\n",
        "\n",
        "    self._position_history.append([action,self._account_balance,t_asset]) # visualize later using the visual bubble. Second variable will be size of bubble.\n",
        "    observation = self._get_observation()\n",
        "    info = dict(\n",
        "        total_profit = self._total_profit,\n",
        "        position = action\n",
        "    )\n",
        "    print(\"step_profit: %f\" % (step_profit))\n",
        "    print(\"current cash: %f\" % (self._account_balance),\"current assets in cash worth: %f\" % (self._invested/self.df.at[self._current_tick,\"Price\"]))\n",
        "    print(\"ACTION: %f\" % (action))\n",
        "    print(\"---------------------\")\n",
        "    self._current_tick += 1\n",
        "    done = False\n",
        "    if self._current_tick==len(map):done = True\n",
        "    if self._account_balance <= 0 and self._invested <=0: done =True\n",
        "    if self._account_balance > 10e8: done = True\n",
        "    if self._account_balance < -10e8: done = True\n",
        "    if self._invested > 10e8: done = True\n",
        "    return observation, step_profit, done, info\n",
        "  \n",
        "  def _trade(self,action):\n",
        "    # should the profit be determined when the sales were made?\n",
        "    c_invested = self._invested/self.df.at[self._current_tick,\"Price\"]\n",
        "    if action <=0:      \n",
        "      t_asset = self._account_balance*action *self.df.at[self._current_tick,\"Price\"]\n",
        "      invested = self._resolution(t_asset)\n",
        "    elif action 0>:\n",
        "      t_asset = self._invested *action *self.df.at[self._current_tick,\"Price\"]\n",
        "    #solve issue of selling amount being too dependent on current cash.\n",
        "    profit =  self._account_balance*action*-1\n",
        "    \n",
        "    return profit,invested,t_asset\n",
        "\n",
        "  def _resolution(self,t_asset):\n",
        "    val1, val2 = False, False\n",
        "    if t_asset >= 0: \n",
        "      val1 = True\n",
        "    if self._invested >= 0: \n",
        "      val2 = True\n",
        "    if val1 == val2 and val1 == True:\n",
        "      new_delta = self._invested + t_asset\n",
        "    elif val1 == val2 and val1 == False:\n",
        "      new_delta = self._invested + t_asset\n",
        "    elif val1 != val2:\n",
        "      if t_asset >= self._invested:\n",
        "        # check setting when value is negative. it is losing money for some reason.\n",
        "        new_delta = abs(t_asset)-abs(self._invested)\n",
        "        if val1 ==False: new_delta=new_delta * -1\n",
        "      elif t_asset < self._invested:\n",
        "       new_delta = abs(self._invested)-abs(t_asset)\n",
        "       if val2 ==False: new_delta =new_delta  * -1\n",
        "    else: print(val1,val2,t_asset,self._invested)\n",
        "    return new_delta\n",
        "\n",
        "\n",
        "  def _get_observation(self):\n",
        "    observation = self.signal_features[(self._current_tick-self.window_size):self._current_tick]\n",
        "    assert observation.shape[0] == 30\n",
        "    return observation\n",
        "\n",
        "  def reset(self):\n",
        "    self._account_balance= self._initial_balance\n",
        "    self._invested = 0\n",
        "    self._total_profit = 0 \n",
        "    self._current_tick = self.window_size\n",
        "    self._position_history = []\n",
        "    self.history = None\n",
        "\n",
        "    observation = self._get_observation()\n",
        "\n",
        "    return observation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "account_balance = 1\n",
        "env = env_trade(map,account_balance=account_balance,max_leverage=0.5)\n",
        "avg_reward = 0.\n",
        "eval_episodes = 1\n",
        "memory=[]\n",
        "for _ in range(eval_episodes):\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, max_actions = env.step(action)\n",
        "    memory.append([obs, reward, done, max_actions])\n",
        "    avg_reward += reward\n",
        "    if env._current_tick>40: done =True\n",
        "avg_reward /= eval_episodes"
      ],
      "metadata": {
        "id": "fl9KfefbpBCt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7loEl_DqdLUB"
      },
      "source": [
        "# `1: Experience memory`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9vkOhPsdS1u"
      },
      "outputs": [],
      "source": [
        "class replay_memory(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkTIDCdOdZTq"
      },
      "source": [
        "# 2: Actor model & Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp6h2n33hnxe"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMBBKre7dhOM"
      },
      "source": [
        "# 3: Two Critic Models & Critics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZfTCtQIt_Mk"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX_DiJiJdnN5"
      },
      "source": [
        "# 4: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FvtOqUP1tZG"
      },
      "outputs": [],
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "class TD3(object):\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    print(self.actor(state).cpu().data.numpy().flatten())\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      batch_rewards = np.array(batch_rewards, dtype=np.float32)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "        #print(next_action.shape,next_state.shape)\n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kvjnC4MdrSW"
      },
      "source": [
        "# 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVW8r8e_YNIy"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, max_actions = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZntwWMrJqmJE"
      },
      "outputs": [],
      "source": [
        "env_name = \"trading\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e5 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw7_gqX1w3Qj"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "account_balance = 1\n",
        "max_lev = 0.5\n",
        "env = env_trade(map,account_balance=account_balance,max_leverage=max_lev)"
      ],
      "metadata": {
        "id": "FPdNk1Ken-3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLTyyPwA2TM9"
      },
      "outputs": [],
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRFoaPQ9Kvpb"
      },
      "outputs": [],
      "source": [
        "# policy net work (not actor critic)\n",
        "policy = TD3(state_dim, action_dim, max_lev)\n",
        "#reply memory creation\n",
        "replay_buffer = replay_memory()\n",
        "# list to store the score of the instance\n",
        "evaluations = [evaluate_policy(policy)]\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlJ8uxCOX6X8"
      },
      "source": [
        "eval_episodes=1\n",
        "avg_reward = 0.\n",
        "for _ in range(eval_episodes):\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    obs, reward, done, max_actions = env.step(action)\n",
        "    avg_reward += reward\n",
        "avg_reward /= eval_episodes\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "print (\"---------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEcm9X8KhkPq"
      },
      "outputs": [],
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()\n",
        "file_name = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYPD-BwiLKpG"
      },
      "outputs": [],
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, max_action = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OsBWwc5_KzVQFvT1-mHP0SVlTc2-gs41",
      "authorship_tag": "ABX9TyMIrGFEL8bZaTt6XsXEorje",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}